import nltk
from langchain_openai import ChatOpenAI
from langchain.chains import ConversationalRetrievalChain
from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import OpenAIEmbeddings
import fitz  # Ø¨Ø±Ø§ÛŒ Ø®ÙˆØ§Ù†Ø¯Ù† PDF
import json
import os
from nltk.tokenize import sent_tokenize
from collections import deque  # Ø¨Ø±Ø§ÛŒ Ù…Ø­Ø¯ÙˆØ¯ Ú©Ø±Ø¯Ù† ØªØ§Ø±ÛŒØ®Ú†Ù‡ Ø¨Ù‡ 10 Ù…ÙˆØ±Ø¯

# ØªÙ†Ø¸ÛŒÙ…Ø§Øª API Key Ø¨Ø±Ø§ÛŒ OpenAI
api_key = os.getenv("API_KEY")

# Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…ØªÙ† Ø§Ø² ÙØ§ÛŒÙ„ PDF
def extract_text_from_pdf(pdf_path):
    if not os.path.exists(pdf_path):
        raise FileNotFoundError("ÙØ§ÛŒÙ„ PDF ÛŒØ§ÙØª Ù†Ø´Ø¯. Ù„Ø·ÙØ§Ù‹ Ù…Ø³ÛŒØ± ÙØ§ÛŒÙ„ Ø±Ø§ Ø¨Ø±Ø±Ø³ÛŒ Ú©Ù†ÛŒØ¯.")
    document = fitz.open(pdf_path)
    text = ""
    for page_num in range(len(document)):
        page = document.load_page(page_num)
        text += page.get_text("text")
    return text

# ØªÙ‚Ø³ÛŒÙ… Ù…ØªÙ† Ø¨Ù‡ Ø¨Ø®Ø´â€ŒÙ‡Ø§ÛŒ Ú©ÙˆÚ†Ú©â€ŒØªØ± Ø¨Ø§ Ø­ÙØ¸ Ø¬Ù…Ù„Ø§Øª
def split_text_into_chunks(text, chunk_size=1000, chunk_overlap=400):
    sentences = sent_tokenize(text)
    chunks, current_chunk = [], ""
    for sentence in sentences:
        if len(current_chunk) + len(sentence) < chunk_size:
            current_chunk += sentence + " "
        else:
            chunks.append(current_chunk.strip())
            current_chunk = sentence + " "
    if current_chunk:
        chunks.append(current_chunk.strip())
    return chunks

# Ø°Ø®ÛŒØ±Ù‡ Ùˆ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù…ØªÙ†â€ŒÙ‡Ø§ÛŒ ØªÙ‚Ø³ÛŒÙ…â€ŒØ´Ø¯Ù‡
def save_chunks_to_json(chunks, json_filename):
    with open(json_filename, 'w', encoding='utf-8') as f:
        json.dump(chunks, f, ensure_ascii=False, indent=4)

def load_chunks_from_json(json_filename):
    if not os.path.exists(json_filename):
        raise FileNotFoundError("ÙØ§ÛŒÙ„ JSON ÙˆØ¬ÙˆØ¯ Ù†Ø¯Ø§Ø±Ø¯. Ù„Ø·ÙØ§Ù‹ Ø§Ø¨ØªØ¯Ø§ ÙØ§ÛŒÙ„ PDF Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø´ÙˆØ¯.")
    with open(json_filename, 'r', encoding='utf-8') as f:
        return json.load(f)

# Ø³Ø§Ø®Øª embedding Ùˆ FAISS
def create_embedding(chunks, api_key):
    embedding = OpenAIEmbeddings(openai_api_key=api_key)
    return FAISS.from_texts(chunks, embedding)  # Ø­Ø°Ù Ù¾Ø§Ø±Ø§Ù…ØªØ± index_type

# Ø°Ø®ÛŒØ±Ù‡ FAISS Index
def save_faiss_index(vector_store, file_name="faiss_index"):
    vector_store.save_local(file_name)

# Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ FAISS Index
def load_faiss_index(file_name="faiss_index"):
    embeddings = OpenAIEmbeddings(openai_api_key=api_key)
    return FAISS.load_local(file_name, embeddings, allow_dangerous_deserialization=True)

# ØªØ§Ø±ÛŒØ®Ú†Ù‡ Ù…Ú©Ø§Ù„Ù…Ù‡ Ø¨Ù‡ ØµÙˆØ±Øª global ÛŒØ§ Ø¯Ø± Ù…ØªØºÛŒØ± Ø³Ø·Ø­ Ø¨Ø§Ù„Ø§
chat_history = deque(maxlen=10)

prompt_template = """
You are "AioMentor" ("Ø¢ÛŒÙˆÙ…Ù†ØªÙˆØ±"). your name is , a friendly, warm, and intelligent assistant that responds in the user's preferred language (default is Persian). 
Your replies should always be concise, clear, and engaging. Ensure to use a good amount of spacing between sentences and points (line breaks) to make responses more readable.
You should use a friendly and casual tone, with a touch of enthusiasm! 
When responding, use bullet points or structured formatting where applicable, especially when listing features or steps. 
Use emojis naturally to enhance engagement and express emotions, but don't overuse them. Keep it light and fun! 
If the user asks a question, always try to give an answer that is not only informative but also encourages further interaction. 
Keep it fun and interactive! Encourage the user to ask more questions or share thoughts. Respond like you're having a casual chat with a friend, and always add a little sparkle to your answers! âœ¨ğŸ¤©

Remember: Your goal is to make every conversation with the user feel friendly, warm, and exciting!
"""

# Ù¾Ø§Ø³Ø® Ø¨Ù‡ Ø³ÙˆØ§Ù„ Ø§Ø² ÙØ§ÛŒÙ„ PDF
def answer_question_from_cached_embedding(pdf_path, question, api_key, json_filename="pdf_chunks.json", index_name="faiss_index"):
    # Ø§Ú¯Ø± FAISS Index Ù…ÙˆØ¬ÙˆØ¯ Ø§Ø³ØªØŒ Ø§Ø² Ø¢Ù† Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†
    if os.path.exists(index_name):
        vector_store = load_faiss_index(index_name)
    else:
        # Ø§Ú¯Ø± ÙØ§ÛŒÙ„ JSON Ù…ÙˆØ¬ÙˆØ¯ Ø§Ø³ØªØŒ Ø§Ø² Ø¢Ù† Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†
        if os.path.exists(json_filename):
            chunks = load_chunks_from_json(json_filename)
        else:
            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ùˆ ØªÙ‚Ø³ÛŒÙ…â€ŒØ¨Ù†Ø¯ÛŒ Ù…ØªÙ† PDF
            text = extract_text_from_pdf(pdf_path)
            chunks = split_text_into_chunks(text)
            save_chunks_to_json(chunks, json_filename)

        # Ø³Ø§Ø®Øª FAISS Index Ùˆ Ø°Ø®ÛŒØ±Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø¢Ù†
        vector_store = create_embedding(chunks, api_key)
        save_faiss_index(vector_store, index_name)

    # Ø³Ø§Ø®Øª Retriever Ø¨Ø±Ø§ÛŒ Ø¬Ø³ØªØ¬Ùˆ Ø¯Ø± FAISS
    retriever = vector_store.as_retriever(search_type="similarity", search_k=1)  # Ú©Ø§Ù‡Ø´ ØªØ¹Ø¯Ø§Ø¯ Ø¬Ø³ØªØ¬ÙˆÙ‡Ø§

    # ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ù…Ø¯Ù„ OpenAI
    llm = ChatOpenAI(openai_api_key=api_key, model_name="gpt-4o", temperature=0.7)  # Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² gpt-4

    # Ø³Ø§Ø®Øª ConversationalRetrievalChain
    chain = ConversationalRetrievalChain.from_llm(llm, retriever)

    # Ù¾Ø±Ø³Ø´ Ø¨Ø§ Ù¾Ø±Ø§Ù…Ù¾Øª
    question_with_prompt = f"{prompt_template}{question}"

    # Ø¯Ø±ÛŒØ§ÙØª Ù¾Ø§Ø³Ø® Ùˆ Ø¨Ù‡â€ŒØ±ÙˆØ² Ú©Ø±Ø¯Ù† ØªØ§Ø±ÛŒØ®Ú†Ù‡
    response = chain.invoke({"question": question_with_prompt, "chat_history": list(chat_history)})

    # Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Ù¾Ø±Ø³Ø´ Ùˆ Ù¾Ø§Ø³Ø® Ø¨Ù‡ ØªØ§Ø±ÛŒØ®Ú†Ù‡ Ø¨Ø±Ø§ÛŒ Ø­ÙØ¸ Ù…Ú©Ø§Ù„Ù…Ù‡
    chat_history.append((question, response["answer"]))

    return response["answer"]
